# Auto-Label Issues with AI (Single-Package Repo)
#
# Automatically classifies new issues with type and priority labels.
#
# Labels applied:
#   - Type: bug, enhancement, documentation
#   - Priority: priority:low, priority:medium, priority:high
#
# Prerequisites:
#   - Delete default labels: "good first issue", "help wanted"
#   - Create priority labels: priority:low, priority:medium, priority:high

name: Auto-Label Issues

on:
  issues:
    types: [opened]

jobs:
  auto-label:
    runs-on: ubuntu-latest
    permissions:
      issues: write
      models: read
      contents: read

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Check if already labelled
        id: check
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          labels=$(gh issue view ${{ github.event.issue.number }} --json labels -q '.labels[].name' 2>/dev/null || echo "")
          if echo "$labels" | grep -qE '^(bug|enhancement|documentation)$'; then
            echo "skip=true" >> $GITHUB_OUTPUT
            echo "Issue already has type label, skipping"
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Run AI assessment
        if: steps.check.outputs.skip != 'true'
        id: ai-assessment
        uses: github/ai-assessment-comment-labeler@main
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          issue_number: ${{ github.event.issue.number }}
          issue_body: ${{ github.event.issue.body }}
          repo_name: ${{ github.event.repository.name }}
          owner: ${{ github.repository_owner }}
          ai_review_label: 'unlabelled'
          prompts_directory: '.github/prompts'
          labels_to_prompts_mapping: 'unlabelled,issue-classification.prompt.yml'
          model: openai/gpt-4o-mini
          max_tokens: 200
          suppress_comments: 'true'
          suppress_labels: 'true'

      - name: Apply derived labels
        if: steps.check.outputs.skip != 'true'
        uses: actions/github-script@v7
        env:
          ASSESSMENT_OUTPUT: ${{ steps.ai-assessment.outputs.ai_assessments }}
        with:
          script: |
            const assessments = JSON.parse(process.env.ASSESSMENT_OUTPUT || '[]');
            if (assessments.length === 0) {
              console.log('No assessments returned');
              return;
            }

            const response = assessments[0].response || '';

            // Parse the structured response
            const typeMatch = response.match(/Type:\s*(bug|enhancement|documentation)/i);
            const priorityMatch = response.match(/Priority:\s*(low|medium|high)/i);

            const labels = [];

            // Add type label (required)
            labels.push(typeMatch ? typeMatch[1].toLowerCase() : 'enhancement');

            // Add priority label (required)
            labels.push(`priority:${priorityMatch ? priorityMatch[1].toLowerCase() : 'medium'}`);

            console.log(`Applying labels: ${labels.join(', ')}`);

            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              labels: labels
            });
